{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import *\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Flatten \n",
    "from keras.layers.convolutional import Conv1D \n",
    "from keras.layers.convolutional import MaxPooling1D \n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data to supervised learning problem\n",
    "def transform_supervised(df, number=5):\n",
    "    i=0\n",
    "    X,y=list(), list()\n",
    "    while i<= (len(df) - number ):\n",
    "        X.append(df[i:i+number].values)\n",
    "        y.append(df[i+number:i+number+1].values)\n",
    "        i=i+1\n",
    "    return X,y\n",
    "    \n",
    "def data_supervised(df, n_in=3, n_out=1, dropna =True):\n",
    "    cols,names =list(), list()\n",
    "    # number of lag\n",
    "    n_vars = df.shape[1]\n",
    "    for i in range( n_in, 0, -1):\n",
    "        cols.append(df.shift(-i))\n",
    "        names+=['var%d(t - %d)' %(j+1, i) for j in range(n_vars)]\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(i))\n",
    "        if i==0:\n",
    "            names+=['var%d(t)' %(j+1) for j in range(n_vars)]\n",
    "        else:\n",
    "            names+=['var%d(t + %d)' %(j+1, i) for j in range(n_vars)]\n",
    "    #concat all dataset\n",
    "    data=pd.concat(cols, axis=1)\n",
    "    data.columns=names\n",
    "    if dropna:\n",
    "        data.dropna(inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "#define the model for MLP\n",
    "def MLP_model(input_dim, optimizer = 'adam', loss='mse'):\n",
    "    model = Sequential() \n",
    "    model.add(Dense(100, activation='tanh', input_dim=3))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1)) \n",
    "    model.compile(optimizer=optimizer, loss=loss) \n",
    "    return model \n",
    "# CNN model\n",
    "def CNN_model():\n",
    "    model = Sequential() \n",
    "    model.add(Conv1D(64, 2, activation='relu', input_shape=(3, 1))) \n",
    "    model.add(MaxPooling1D()) \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(100, activation='tanh'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(1)) \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-68aa33d79b52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# transform data to dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# transform data to dataframe\n",
    "\n",
    "fh = open('')\n",
    "\n",
    "num_list = []\n",
    "\n",
    "read_lines = fh.readlines()\n",
    "for line in read_lines:\n",
    "    num_list.append(line)\n",
    "\n",
    "fh.close()\n",
    "data = num_list[0].split(' ')\n",
    "Date=list()\n",
    "Births= list()\n",
    "for num in data:\n",
    "    Date.append(num.split(',')[0][1:-1])\n",
    "    Births.append(num.split(',')[1])\n",
    "Date=Date[1:]\n",
    "Births=Births[1:]\n",
    "\n",
    "total_daily=pd.DataFrame({'Date':Date, 'Births': Births})\n",
    "total_daily['Date']=pd.to_datetime(total_daily.Date, format='%Y-%m-%d')\n",
    "total_daily.index=total_daily.Date\n",
    "total_daily=total_daily.drop(['Date'], axis=1)\n",
    "total_daily.to_csv(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 How to transform data to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t - 3)</th>\n",
       "      <th>var1(t - 2)</th>\n",
       "      <th>var1(t - 1)</th>\n",
       "      <th>var1(t)</th>\n",
       "      <th>var1(t + 1)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959-01-02</th>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-01-03</th>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-01-04</th>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-01-05</th>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1(t - 3) var1(t - 2) var1(t - 1) var1(t) var1(t + 1)\n",
       "Date                                                              \n",
       "1959-01-02          44          31          30      32          35\n",
       "1959-01-03          29          44          31      30          32\n",
       "1959-01-04          45          29          44      31          30\n",
       "1959-01-05          43          45          29      44          31"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_supervised(total_daily, n_in=3, n_out=2, dropna =True).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. MLP for time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t - 3)</th>\n",
       "      <th>var1(t - 2)</th>\n",
       "      <th>var1(t - 1)</th>\n",
       "      <th>var1(t)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1959-01-01</th>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-01-02</th>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-01-03</th>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959-01-04</th>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           var1(t - 3) var1(t - 2) var1(t - 1) var1(t)\n",
       "Date                                                  \n",
       "1959-01-01          31          30          32      35\n",
       "1959-01-02          44          31          30      32\n",
       "1959-01-03          29          44          31      30\n",
       "1959-01-04          45          29          44      31"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data_supervised(total_daily, n_in=3, n_out=1, dropna =True)\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           var1(t)\n",
      "Date              \n",
      "1959-10-17      49\n",
      "1959-10-18      45\n",
      "1959-10-19      43\n",
      "1959-10-20      42\n",
      "1959-10-21      38\n",
      "...            ...\n",
      "1959-12-24      38\n",
      "1959-12-25      44\n",
      "1959-12-26      34\n",
      "1959-12-27      37\n",
      "1959-12-28      52\n",
      "\n",
      "[73 rows x 1 columns]\n",
      "           var1(t - 3) var1(t - 2) var1(t - 1)\n",
      "Date                                          \n",
      "1959-10-17          42          43          45\n",
      "1959-10-18          38          42          43\n",
      "1959-10-19          47          38          42\n",
      "1959-10-20          38          47          38\n",
      "1959-10-21          36          38          47\n",
      "...                ...         ...         ...\n",
      "1959-12-24          37          34          44\n",
      "1959-12-25          52          37          34\n",
      "1959-12-26          48          52          37\n",
      "1959-12-27          55          48          52\n",
      "1959-12-28        50\\n          55          48\n",
      "\n",
      "[73 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#define the dataset\n",
    "n_train=int(len(data)*0.8)\n",
    "X_train=data.iloc[:,0:-1][0:n_train]\n",
    "X_test=data.iloc[:,0:-1][n_train:]\n",
    "y_train=data.iloc[:,[-1]][0:n_train]\n",
    "y_test = data.iloc[:,[-1]][n_train:]\n",
    "print(y_test)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MLP_model(3)\n",
    "#model.fit(X_train, y_train, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.953957928825948"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict X_test with the train model\n",
    "y_pred = model.predict(X_test)\n",
    "error=np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. CNN for time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['44'],\n",
       "       ['31'],\n",
       "       ['30']], dtype=object)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values.reshape(X_train.shape[0], X_train.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=int(len(data)*0.8)\n",
    "X_train=data.iloc[:,0:-1][0:n_train].values.reshape(X_train.shape[0], X_train.shape[1],1)\n",
    "X_test=data.iloc[:,0:-1][n_train:].values.reshape(X_test.shape[0], X_test.shape[1],1)\n",
    "y_train=data.iloc[:,[-1]][0:n_train]\n",
    "y_test = data.iloc[:,[-1]][n_train:]\n",
    "#print(y_test)\n",
    "#print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "289/289 [==============================] - 0s 1ms/step - loss: 1376.6096\n",
      "Epoch 2/200\n",
      "289/289 [==============================] - 0s 62us/step - loss: 890.4478\n",
      "Epoch 3/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 526.9384\n",
      "Epoch 4/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 255.5103\n",
      "Epoch 5/200\n",
      "289/289 [==============================] - 0s 73us/step - loss: 109.8710\n",
      "Epoch 6/200\n",
      "289/289 [==============================] - 0s 72us/step - loss: 59.9245\n",
      "Epoch 7/200\n",
      "289/289 [==============================] - 0s 79us/step - loss: 57.7810\n",
      "Epoch 8/200\n",
      "289/289 [==============================] - 0s 72us/step - loss: 58.2984\n",
      "Epoch 9/200\n",
      "289/289 [==============================] - 0s 72us/step - loss: 57.8299\n",
      "Epoch 10/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 57.4001\n",
      "Epoch 11/200\n",
      "289/289 [==============================] - 0s 79us/step - loss: 57.7241\n",
      "Epoch 12/200\n",
      "289/289 [==============================] - 0s 83us/step - loss: 57.0932\n",
      "Epoch 13/200\n",
      "289/289 [==============================] - 0s 86us/step - loss: 57.0926\n",
      "Epoch 14/200\n",
      "289/289 [==============================] - 0s 124us/step - loss: 57.2895\n",
      "Epoch 15/200\n",
      "289/289 [==============================] - 0s 124us/step - loss: 57.1253\n",
      "Epoch 16/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 57.5397\n",
      "Epoch 17/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 57.5110\n",
      "Epoch 18/200\n",
      "289/289 [==============================] - 0s 107us/step - loss: 57.9320\n",
      "Epoch 19/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 58.1013\n",
      "Epoch 20/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 57.2500\n",
      "Epoch 21/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 57.1098\n",
      "Epoch 22/200\n",
      "289/289 [==============================] - 0s 107us/step - loss: 57.1589\n",
      "Epoch 23/200\n",
      "289/289 [==============================] - 0s 121us/step - loss: 57.2380\n",
      "Epoch 24/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 57.3055\n",
      "Epoch 25/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 57.2241\n",
      "Epoch 26/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 58.2767\n",
      "Epoch 27/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 59.5224\n",
      "Epoch 28/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 58.9405\n",
      "Epoch 29/200\n",
      "289/289 [==============================] - 0s 145us/step - loss: 57.4086\n",
      "Epoch 30/200\n",
      "289/289 [==============================] - 0s 157us/step - loss: 57.0199\n",
      "Epoch 31/200\n",
      "289/289 [==============================] - 0s 150us/step - loss: 56.9537\n",
      "Epoch 32/200\n",
      "289/289 [==============================] - 0s 153us/step - loss: 57.1961\n",
      "Epoch 33/200\n",
      "289/289 [==============================] - 0s 168us/step - loss: 57.0248\n",
      "Epoch 34/200\n",
      "289/289 [==============================] - 0s 174us/step - loss: 56.8279\n",
      "Epoch 35/200\n",
      "289/289 [==============================] - 0s 176us/step - loss: 56.9021\n",
      "Epoch 36/200\n",
      "289/289 [==============================] - 0s 156us/step - loss: 57.2744\n",
      "Epoch 37/200\n",
      "289/289 [==============================] - 0s 137us/step - loss: 56.6926\n",
      "Epoch 38/200\n",
      "289/289 [==============================] - 0s 135us/step - loss: 57.1324\n",
      "Epoch 39/200\n",
      "289/289 [==============================] - 0s 145us/step - loss: 56.9581\n",
      "Epoch 40/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 56.8232\n",
      "Epoch 41/200\n",
      "289/289 [==============================] - 0s 121us/step - loss: 58.6697\n",
      "Epoch 42/200\n",
      "289/289 [==============================] - 0s 143us/step - loss: 57.0742\n",
      "Epoch 43/200\n",
      "289/289 [==============================] - 0s 135us/step - loss: 57.1880\n",
      "Epoch 44/200\n",
      "289/289 [==============================] - 0s 124us/step - loss: 58.2756\n",
      "Epoch 45/200\n",
      "289/289 [==============================] - 0s 135us/step - loss: 56.9358\n",
      "Epoch 46/200\n",
      "289/289 [==============================] - 0s 155us/step - loss: 61.1000\n",
      "Epoch 47/200\n",
      "289/289 [==============================] - 0s 154us/step - loss: 56.6943\n",
      "Epoch 48/200\n",
      "289/289 [==============================] - 0s 179us/step - loss: 56.8188\n",
      "Epoch 49/200\n",
      "289/289 [==============================] - 0s 180us/step - loss: 57.0459\n",
      "Epoch 50/200\n",
      "289/289 [==============================] - 0s 168us/step - loss: 59.0358\n",
      "Epoch 51/200\n",
      "289/289 [==============================] - 0s 156us/step - loss: 57.0660\n",
      "Epoch 52/200\n",
      "289/289 [==============================] - 0s 155us/step - loss: 57.0771\n",
      "Epoch 53/200\n",
      "289/289 [==============================] - 0s 147us/step - loss: 58.4259\n",
      "Epoch 54/200\n",
      "289/289 [==============================] - 0s 147us/step - loss: 57.8669\n",
      "Epoch 55/200\n",
      "289/289 [==============================] - 0s 131us/step - loss: 57.0860\n",
      "Epoch 56/200\n",
      "289/289 [==============================] - 0s 124us/step - loss: 57.4416\n",
      "Epoch 57/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 56.5939\n",
      "Epoch 58/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 56.6452\n",
      "Epoch 59/200\n",
      "289/289 [==============================] - 0s 150us/step - loss: 56.4324\n",
      "Epoch 60/200\n",
      "289/289 [==============================] - 0s 162us/step - loss: 57.7388\n",
      "Epoch 61/200\n",
      "289/289 [==============================] - 0s 162us/step - loss: 56.9923\n",
      "Epoch 62/200\n",
      "289/289 [==============================] - 0s 180us/step - loss: 56.6845\n",
      "Epoch 63/200\n",
      "289/289 [==============================] - 0s 187us/step - loss: 56.5102\n",
      "Epoch 64/200\n",
      "289/289 [==============================] - ETA: 0s - loss: 37.09 - 0s 198us/step - loss: 57.3944\n",
      "Epoch 65/200\n",
      "289/289 [==============================] - 0s 163us/step - loss: 57.6279\n",
      "Epoch 66/200\n",
      "289/289 [==============================] - 0s 159us/step - loss: 56.7713\n",
      "Epoch 67/200\n",
      "289/289 [==============================] - 0s 166us/step - loss: 56.9181\n",
      "Epoch 68/200\n",
      "289/289 [==============================] - 0s 162us/step - loss: 57.0188\n",
      "Epoch 69/200\n",
      "289/289 [==============================] - 0s 175us/step - loss: 57.4592\n",
      "Epoch 70/200\n",
      "289/289 [==============================] - 0s 162us/step - loss: 58.5457\n",
      "Epoch 71/200\n",
      "289/289 [==============================] - 0s 169us/step - loss: 57.3318\n",
      "Epoch 72/200\n",
      "289/289 [==============================] - 0s 174us/step - loss: 57.0311\n",
      "Epoch 73/200\n",
      "289/289 [==============================] - 0s 158us/step - loss: 57.4890\n",
      "Epoch 74/200\n",
      "289/289 [==============================] - 0s 121us/step - loss: 58.3481\n",
      "Epoch 75/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 57.8847\n",
      "Epoch 76/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 56.9432\n",
      "Epoch 77/200\n",
      "289/289 [==============================] - 0s 145us/step - loss: 57.1336\n",
      "Epoch 78/200\n",
      "289/289 [==============================] - 0s 131us/step - loss: 58.0036\n",
      "Epoch 79/200\n",
      "289/289 [==============================] - 0s 131us/step - loss: 58.6423\n",
      "Epoch 80/200\n",
      "289/289 [==============================] - 0s 172us/step - loss: 57.5921\n",
      "Epoch 81/200\n",
      "289/289 [==============================] - 0s 170us/step - loss: 56.9652\n",
      "Epoch 82/200\n",
      "289/289 [==============================] - 0s 178us/step - loss: 57.6658\n",
      "Epoch 83/200\n",
      "289/289 [==============================] - 0s 180us/step - loss: 56.7832\n",
      "Epoch 84/200\n",
      "289/289 [==============================] - 0s 172us/step - loss: 57.2673\n",
      "Epoch 85/200\n",
      "289/289 [==============================] - 0s 165us/step - loss: 56.7121\n",
      "Epoch 86/200\n",
      "289/289 [==============================] - 0s 148us/step - loss: 57.1640\n",
      "Epoch 87/200\n",
      "289/289 [==============================] - 0s 134us/step - loss: 63.4961\n",
      "Epoch 88/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 58.8421\n",
      "Epoch 89/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 57.3813\n",
      "Epoch 90/200\n",
      "289/289 [==============================] - 0s 107us/step - loss: 57.9781\n",
      "Epoch 91/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 56.3783\n",
      "Epoch 92/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 57.3596\n",
      "Epoch 93/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 56.7032\n",
      "Epoch 94/200\n",
      "289/289 [==============================] - 0s 124us/step - loss: 56.8777\n",
      "Epoch 95/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 58.7870\n",
      "Epoch 96/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 0s 110us/step - loss: 56.5525\n",
      "Epoch 97/200\n",
      "289/289 [==============================] - 0s 124us/step - loss: 57.2835\n",
      "Epoch 98/200\n",
      "289/289 [==============================] - 0s 121us/step - loss: 57.3316\n",
      "Epoch 99/200\n",
      "289/289 [==============================] - 0s 162us/step - loss: 56.8040\n",
      "Epoch 100/200\n",
      "289/289 [==============================] - 0s 168us/step - loss: 56.4965\n",
      "Epoch 101/200\n",
      "289/289 [==============================] - 0s 189us/step - loss: 56.3185\n",
      "Epoch 102/200\n",
      "289/289 [==============================] - 0s 169us/step - loss: 56.3809\n",
      "Epoch 103/200\n",
      "289/289 [==============================] - 0s 184us/step - loss: 55.7769\n",
      "Epoch 104/200\n",
      "289/289 [==============================] - 0s 177us/step - loss: 55.7613\n",
      "Epoch 105/200\n",
      "289/289 [==============================] - 0s 167us/step - loss: 56.1691\n",
      "Epoch 106/200\n",
      "289/289 [==============================] - 0s 184us/step - loss: 55.2780\n",
      "Epoch 107/200\n",
      "289/289 [==============================] - 0s 150us/step - loss: 57.2655\n",
      "Epoch 108/200\n",
      "289/289 [==============================] - 0s 152us/step - loss: 55.7216\n",
      "Epoch 109/200\n",
      "289/289 [==============================] - 0s 138us/step - loss: 56.3787\n",
      "Epoch 110/200\n",
      "289/289 [==============================] - 0s 183us/step - loss: 55.9824\n",
      "Epoch 111/200\n",
      "289/289 [==============================] - 0s 183us/step - loss: 56.7658\n",
      "Epoch 112/200\n",
      "289/289 [==============================] - 0s 184us/step - loss: 59.1417\n",
      "Epoch 113/200\n",
      "289/289 [==============================] - 0s 183us/step - loss: 57.2553\n",
      "Epoch 114/200\n",
      "289/289 [==============================] - 0s 186us/step - loss: 56.7159\n",
      "Epoch 115/200\n",
      "289/289 [==============================] - 0s 189us/step - loss: 56.4594\n",
      "Epoch 116/200\n",
      "289/289 [==============================] - 0s 195us/step - loss: 56.7825\n",
      "Epoch 117/200\n",
      "289/289 [==============================] - 0s 168us/step - loss: 58.9553\n",
      "Epoch 118/200\n",
      "289/289 [==============================] - 0s 145us/step - loss: 56.4994\n",
      "Epoch 119/200\n",
      "289/289 [==============================] - 0s 162us/step - loss: 56.8846\n",
      "Epoch 120/200\n",
      "289/289 [==============================] - 0s 166us/step - loss: 59.3149\n",
      "Epoch 121/200\n",
      "289/289 [==============================] - 0s 145us/step - loss: 58.0486\n",
      "Epoch 122/200\n",
      "289/289 [==============================] - 0s 128us/step - loss: 59.3577\n",
      "Epoch 123/200\n",
      "289/289 [==============================] - 0s 176us/step - loss: 56.3796\n",
      "Epoch 124/200\n",
      "289/289 [==============================] - 0s 191us/step - loss: 56.7707\n",
      "Epoch 125/200\n",
      "289/289 [==============================] - 0s 166us/step - loss: 56.6124\n",
      "Epoch 126/200\n",
      "289/289 [==============================] - 0s 188us/step - loss: 57.2869\n",
      "Epoch 127/200\n",
      "289/289 [==============================] - 0s 187us/step - loss: 60.7902\n",
      "Epoch 128/200\n",
      "289/289 [==============================] - 0s 167us/step - loss: 56.5133\n",
      "Epoch 129/200\n",
      "289/289 [==============================] - 0s 136us/step - loss: 56.1642\n",
      "Epoch 130/200\n",
      "289/289 [==============================] - 0s 154us/step - loss: 56.7912\n",
      "Epoch 131/200\n",
      "289/289 [==============================] - 0s 145us/step - loss: 56.4626\n",
      "Epoch 132/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 55.8582\n",
      "Epoch 133/200\n",
      "289/289 [==============================] - 0s 138us/step - loss: 56.0534\n",
      "Epoch 134/200\n",
      "289/289 [==============================] - 0s 138us/step - loss: 56.5065\n",
      "Epoch 135/200\n",
      "289/289 [==============================] - 0s 166us/step - loss: 57.6714\n",
      "Epoch 136/200\n",
      "289/289 [==============================] - 0s 185us/step - loss: 55.8097\n",
      "Epoch 137/200\n",
      "289/289 [==============================] - 0s 180us/step - loss: 56.7668\n",
      "Epoch 138/200\n",
      "289/289 [==============================] - 0s 152us/step - loss: 58.2667\n",
      "Epoch 139/200\n",
      "289/289 [==============================] - 0s 206us/step - loss: 56.9531\n",
      "Epoch 140/200\n",
      "289/289 [==============================] - 0s 239us/step - loss: 56.1729\n",
      "Epoch 141/200\n",
      "289/289 [==============================] - 0s 182us/step - loss: 56.1794\n",
      "Epoch 142/200\n",
      "289/289 [==============================] - 0s 150us/step - loss: 67.0041\n",
      "Epoch 143/200\n",
      "289/289 [==============================] - 0s 141us/step - loss: 58.8678\n",
      "Epoch 144/200\n",
      "289/289 [==============================] - 0s 128us/step - loss: 55.8672\n",
      "Epoch 145/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 59.8813\n",
      "Epoch 146/200\n",
      "289/289 [==============================] - 0s 107us/step - loss: 56.9200\n",
      "Epoch 147/200\n",
      "289/289 [==============================] - 0s 131us/step - loss: 57.1945\n",
      "Epoch 148/200\n",
      "289/289 [==============================] - 0s 129us/step - loss: 57.3207\n",
      "Epoch 149/200\n",
      "289/289 [==============================] - 0s 141us/step - loss: 63.8267\n",
      "Epoch 150/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 61.8113\n",
      "Epoch 151/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 58.4717\n",
      "Epoch 152/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 57.6569\n",
      "Epoch 153/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 56.6298\n",
      "Epoch 154/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 56.6727\n",
      "Epoch 155/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 56.2648\n",
      "Epoch 156/200\n",
      "289/289 [==============================] - 0s 107us/step - loss: 57.0027\n",
      "Epoch 157/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 55.8883\n",
      "Epoch 158/200\n",
      "289/289 [==============================] - 0s 117us/step - loss: 55.7626\n",
      "Epoch 159/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 56.6446\n",
      "Epoch 160/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 56.0470\n",
      "Epoch 161/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 55.9613\n",
      "Epoch 162/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 55.8506\n",
      "Epoch 163/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 56.7264\n",
      "Epoch 164/200\n",
      "289/289 [==============================] - 0s 114us/step - loss: 55.9538\n",
      "Epoch 165/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 57.4241\n",
      "Epoch 166/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 59.3579\n",
      "Epoch 167/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 56.1362\n",
      "Epoch 168/200\n",
      "289/289 [==============================] - 0s 90us/step - loss: 55.8083\n",
      "Epoch 169/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 56.5126\n",
      "Epoch 170/200\n",
      "289/289 [==============================] - 0s 93us/step - loss: 57.3309\n",
      "Epoch 171/200\n",
      "289/289 [==============================] - 0s 93us/step - loss: 55.7723\n",
      "Epoch 172/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 55.4153\n",
      "Epoch 173/200\n",
      "289/289 [==============================] - 0s 86us/step - loss: 55.7851\n",
      "Epoch 174/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 55.4598\n",
      "Epoch 175/200\n",
      "289/289 [==============================] - 0s 90us/step - loss: 55.4469\n",
      "Epoch 176/200\n",
      "289/289 [==============================] - 0s 79us/step - loss: 56.0879\n",
      "Epoch 177/200\n",
      "289/289 [==============================] - 0s 83us/step - loss: 55.6995\n",
      "Epoch 178/200\n",
      "289/289 [==============================] - 0s 79us/step - loss: 55.4904\n",
      "Epoch 179/200\n",
      "289/289 [==============================] - 0s 83us/step - loss: 55.8287\n",
      "Epoch 180/200\n",
      "289/289 [==============================] - 0s 104us/step - loss: 57.6251\n",
      "Epoch 181/200\n",
      "289/289 [==============================] - 0s 79us/step - loss: 55.2186\n",
      "Epoch 182/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 57.7169\n",
      "Epoch 183/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 56.6418\n",
      "Epoch 184/200\n",
      "289/289 [==============================] - 0s 83us/step - loss: 57.1189\n",
      "Epoch 185/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 56.7591\n",
      "Epoch 186/200\n",
      "289/289 [==============================] - 0s 83us/step - loss: 55.7514\n",
      "Epoch 187/200\n",
      "289/289 [==============================] - 0s 79us/step - loss: 55.5054\n",
      "Epoch 188/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 55.1530\n",
      "Epoch 189/200\n",
      "289/289 [==============================] - 0s 93us/step - loss: 61.4199\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 0s 97us/step - loss: 60.0276\n",
      "Epoch 191/200\n",
      "289/289 [==============================] - 0s 76us/step - loss: 57.1398\n",
      "Epoch 192/200\n",
      "289/289 [==============================] - 0s 93us/step - loss: 58.1243\n",
      "Epoch 193/200\n",
      "289/289 [==============================] - 0s 97us/step - loss: 55.4535\n",
      "Epoch 194/200\n",
      "289/289 [==============================] - 0s 100us/step - loss: 56.7413\n",
      "Epoch 195/200\n",
      "289/289 [==============================] - 0s 86us/step - loss: 55.2249\n",
      "Epoch 196/200\n",
      "289/289 [==============================] - 0s 72us/step - loss: 55.8518\n",
      "Epoch 197/200\n",
      "289/289 [==============================] - 0s 83us/step - loss: 56.1212\n",
      "Epoch 198/200\n",
      "289/289 [==============================] - 0s 110us/step - loss: 55.7195\n",
      "Epoch 199/200\n",
      "289/289 [==============================] - 0s 99us/step - loss: 54.8181\n",
      "Epoch 200/200\n",
      "289/289 [==============================] - 0s 96us/step - loss: 55.5490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d218789ac8>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "CNN_model=CNN_model()\n",
    "CNN_model.fit(X_train, y_train, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.190013784519519"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred= CNN_model.predict(X_test)\n",
    "error=np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. LSTM for time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "289/289 [==============================] - 1s 4ms/step - loss: 1611.1212\n",
      "Epoch 2/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 1198.0863\n",
      "Epoch 3/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 384.3250\n",
      "Epoch 4/100\n",
      "289/289 [==============================] - 0s 280us/step - loss: 166.3086\n",
      "Epoch 5/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 85.1133\n",
      "Epoch 6/100\n",
      "289/289 [==============================] - 0s 293us/step - loss: 88.6333\n",
      "Epoch 7/100\n",
      "289/289 [==============================] - 0s 338us/step - loss: 75.5509\n",
      "Epoch 8/100\n",
      "289/289 [==============================] - 0s 286us/step - loss: 74.2405\n",
      "Epoch 9/100\n",
      "289/289 [==============================] - 0s 290us/step - loss: 71.7412\n",
      "Epoch 10/100\n",
      "289/289 [==============================] - 0s 297us/step - loss: 68.8704\n",
      "Epoch 11/100\n",
      "289/289 [==============================] - 0s 307us/step - loss: 73.3427\n",
      "Epoch 12/100\n",
      "289/289 [==============================] - 0s 286us/step - loss: 67.3199\n",
      "Epoch 13/100\n",
      "289/289 [==============================] - 0s 231us/step - loss: 64.7261\n",
      "Epoch 14/100\n",
      "289/289 [==============================] - 0s 235us/step - loss: 72.4846\n",
      "Epoch 15/100\n",
      "289/289 [==============================] - 0s 276us/step - loss: 63.9321\n",
      "Epoch 16/100\n",
      "289/289 [==============================] - 0s 252us/step - loss: 63.3018\n",
      "Epoch 17/100\n",
      "289/289 [==============================] - 0s 300us/step - loss: 62.0258\n",
      "Epoch 18/100\n",
      "289/289 [==============================] - 0s 300us/step - loss: 57.6799\n",
      "Epoch 19/100\n",
      "289/289 [==============================] - 0s 297us/step - loss: 57.0474\n",
      "Epoch 20/100\n",
      "289/289 [==============================] - 0s 286us/step - loss: 57.1559\n",
      "Epoch 21/100\n",
      "289/289 [==============================] - 0s 290us/step - loss: 59.9011\n",
      "Epoch 22/100\n",
      "289/289 [==============================] - 0s 293us/step - loss: 59.5278\n",
      "Epoch 23/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 55.3694\n",
      "Epoch 24/100\n",
      "289/289 [==============================] - 0s 390us/step - loss: 57.4603\n",
      "Epoch 25/100\n",
      "289/289 [==============================] - 0s 380us/step - loss: 55.8282\n",
      "Epoch 26/100\n",
      "289/289 [==============================] - 0s 352us/step - loss: 59.2808\n",
      "Epoch 27/100\n",
      "289/289 [==============================] - 0s 335us/step - loss: 64.6394\n",
      "Epoch 28/100\n",
      "289/289 [==============================] - 0s 362us/step - loss: 58.0466\n",
      "Epoch 29/100\n",
      "289/289 [==============================] - 0s 362us/step - loss: 55.2315\n",
      "Epoch 30/100\n",
      "289/289 [==============================] - 0s 338us/step - loss: 54.7711\n",
      "Epoch 31/100\n",
      "289/289 [==============================] - 0s 369us/step - loss: 56.9844\n",
      "Epoch 32/100\n",
      "289/289 [==============================] - 0s 362us/step - loss: 54.4726\n",
      "Epoch 33/100\n",
      "289/289 [==============================] - 0s 349us/step - loss: 58.5246\n",
      "Epoch 34/100\n",
      "289/289 [==============================] - 0s 335us/step - loss: 58.5815\n",
      "Epoch 35/100\n",
      "289/289 [==============================] - 0s 380us/step - loss: 63.2904\n",
      "Epoch 36/100\n",
      "289/289 [==============================] - 0s 393us/step - loss: 59.1491\n",
      "Epoch 37/100\n",
      "289/289 [==============================] - 0s 342us/step - loss: 55.7463\n",
      "Epoch 38/100\n",
      "289/289 [==============================] - 0s 335us/step - loss: 54.7683\n",
      "Epoch 39/100\n",
      "289/289 [==============================] - 0s 359us/step - loss: 54.7203\n",
      "Epoch 40/100\n",
      "289/289 [==============================] - 0s 338us/step - loss: 54.9578\n",
      "Epoch 41/100\n",
      "289/289 [==============================] - 0s 342us/step - loss: 55.2441\n",
      "Epoch 42/100\n",
      "289/289 [==============================] - 0s 324us/step - loss: 58.0910\n",
      "Epoch 43/100\n",
      "289/289 [==============================] - 0s 307us/step - loss: 66.1904\n",
      "Epoch 44/100\n",
      "289/289 [==============================] - 0s 314us/step - loss: 54.2934\n",
      "Epoch 45/100\n",
      "289/289 [==============================] - 0s 331us/step - loss: 55.8784\n",
      "Epoch 46/100\n",
      "289/289 [==============================] - 0s 324us/step - loss: 55.2310\n",
      "Epoch 47/100\n",
      "289/289 [==============================] - 0s 324us/step - loss: 54.3464\n",
      "Epoch 48/100\n",
      "289/289 [==============================] - 0s 362us/step - loss: 54.4108\n",
      "Epoch 49/100\n",
      "289/289 [==============================] - 0s 331us/step - loss: 54.8671\n",
      "Epoch 50/100\n",
      "289/289 [==============================] - 0s 314us/step - loss: 54.4365\n",
      "Epoch 51/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 54.5265\n",
      "Epoch 52/100\n",
      "289/289 [==============================] - 0s 362us/step - loss: 54.4791\n",
      "Epoch 53/100\n",
      "289/289 [==============================] - 0s 355us/step - loss: 68.8013\n",
      "Epoch 54/100\n",
      "289/289 [==============================] - 0s 328us/step - loss: 57.4096\n",
      "Epoch 55/100\n",
      "289/289 [==============================] - 0s 317us/step - loss: 54.9778\n",
      "Epoch 56/100\n",
      "289/289 [==============================] - 0s 345us/step - loss: 55.3449\n",
      "Epoch 57/100\n",
      "289/289 [==============================] - 0s 362us/step - loss: 54.5490\n",
      "Epoch 58/100\n",
      "289/289 [==============================] - 0s 355us/step - loss: 57.0628\n",
      "Epoch 59/100\n",
      "289/289 [==============================] - 0s 342us/step - loss: 55.0882\n",
      "Epoch 60/100\n",
      "289/289 [==============================] - 0s 359us/step - loss: 54.1638\n",
      "Epoch 61/100\n",
      "289/289 [==============================] - 0s 328us/step - loss: 59.7584\n",
      "Epoch 62/100\n",
      "289/289 [==============================] - 0s 324us/step - loss: 66.4555\n",
      "Epoch 63/100\n",
      "289/289 [==============================] - 0s 338us/step - loss: 60.4404\n",
      "Epoch 64/100\n",
      "289/289 [==============================] - 0s 290us/step - loss: 57.0506\n",
      "Epoch 65/100\n",
      "289/289 [==============================] - 0s 297us/step - loss: 56.3831\n",
      "Epoch 66/100\n",
      "289/289 [==============================] - 0s 276us/step - loss: 56.1243\n",
      "Epoch 67/100\n",
      "289/289 [==============================] - 0s 276us/step - loss: 56.8804\n",
      "Epoch 68/100\n",
      "289/289 [==============================] - 0s 283us/step - loss: 54.8284\n",
      "Epoch 69/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 57.2328\n",
      "Epoch 70/100\n",
      "289/289 [==============================] - 0s 293us/step - loss: 54.6813\n",
      "Epoch 71/100\n",
      "289/289 [==============================] - 0s 266us/step - loss: 58.3716\n",
      "Epoch 72/100\n",
      "289/289 [==============================] - 0s 252us/step - loss: 59.6012\n",
      "Epoch 73/100\n",
      "289/289 [==============================] - 0s 276us/step - loss: 57.0904\n",
      "Epoch 74/100\n",
      "289/289 [==============================] - 0s 266us/step - loss: 56.4104\n",
      "Epoch 75/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 55.7614\n",
      "Epoch 76/100\n",
      "289/289 [==============================] - 0s 290us/step - loss: 54.3661\n",
      "Epoch 77/100\n",
      "289/289 [==============================] - 0s 266us/step - loss: 55.1460\n",
      "Epoch 78/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 54.9856\n",
      "Epoch 79/100\n",
      "289/289 [==============================] - 0s 286us/step - loss: 55.7100\n",
      "Epoch 80/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 55.7616\n",
      "Epoch 81/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 54.4806\n",
      "Epoch 82/100\n",
      "289/289 [==============================] - 0s 255us/step - loss: 70.0404\n",
      "Epoch 83/100\n",
      "289/289 [==============================] - 0s 286us/step - loss: 69.0484\n",
      "Epoch 84/100\n",
      "289/289 [==============================] - 0s 276us/step - loss: 59.8967\n",
      "Epoch 85/100\n",
      "289/289 [==============================] - ETA: 0s - loss: 60.85 - 0s 266us/step - loss: 62.7580\n",
      "Epoch 86/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 56.7161\n",
      "Epoch 87/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 55.4941\n",
      "Epoch 88/100\n",
      "289/289 [==============================] - 0s 280us/step - loss: 56.2850\n",
      "Epoch 89/100\n",
      "289/289 [==============================] - 0s 280us/step - loss: 57.1880\n",
      "Epoch 90/100\n",
      "289/289 [==============================] - 0s 276us/step - loss: 56.5940\n",
      "Epoch 91/100\n",
      "289/289 [==============================] - 0s 290us/step - loss: 56.2584\n",
      "Epoch 92/100\n",
      "289/289 [==============================] - 0s 297us/step - loss: 54.7521\n",
      "Epoch 93/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 54.3293\n",
      "Epoch 94/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 56.6709\n",
      "Epoch 95/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 55.8358\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289/289 [==============================] - 0s 269us/step - loss: 57.6489\n",
      "Epoch 97/100\n",
      "289/289 [==============================] - 0s 252us/step - loss: 57.3779\n",
      "Epoch 98/100\n",
      "289/289 [==============================] - 0s 300us/step - loss: 55.7050\n",
      "Epoch 99/100\n",
      "289/289 [==============================] - 0s 273us/step - loss: 54.3921\n",
      "Epoch 100/100\n",
      "289/289 [==============================] - 0s 269us/step - loss: 56.1045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d21d8e6a48>"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(LSTM(100, activation='relu', input_shape=(3, 1))) \n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "# fit model \n",
    "model.fit(X_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.025848815418112"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred= model.predict(X_test)\n",
    "error=np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
